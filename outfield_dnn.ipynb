{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/bravespositioning/braves_23_sim.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfield_data = df[(df['is_fb'] == 1)]\n",
    "outfield_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame with the data loaded\n",
    "# Define the features and target\n",
    "features = [\n",
    "    'responsible_fielder',\n",
    "    'fielder_landing_x', 'fielder_landing_y', 'fielder_landing_angle_radians',\n",
    "    'launch_speed', 'launch_angle', 'hit_direction', 'hangtime', \n",
    "    'landing_x', 'landing_y', 'responsible_fielder_x', 'responsible_fielder_y',\n",
    "]\n",
    "target = 'is_out'\n",
    "\n",
    "# Separate features and target\n",
    "X = df[features].copy()  # Avoid modifying the original DataFrame\n",
    "y = df[target]\n",
    "\n",
    "# Encode the categorical feature `responsible_fielder`\n",
    "label_encoder = LabelEncoder()\n",
    "X['responsible_fielder'] = label_encoder.fit_transform(X['responsible_fielder'])\n",
    "\n",
    "# Define numeric features\n",
    "numeric_features = [\n",
    "    'fielder_landing_x', 'fielder_landing_y', 'fielder_landing_angle_radians',\n",
    "    'launch_speed', 'launch_angle', 'hit_direction', 'hangtime', \n",
    "    'landing_x', 'landing_y', 'responsible_fielder_x', 'responsible_fielder_y',\n",
    "]\n",
    "\n",
    "# Create a preprocessing pipeline for numeric data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to numeric features\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.002), input_shape=(X_train_processed.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Use a static learning rate\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=initial_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_processed, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_processed, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Plotting the training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate predicted probabilities\n",
    "y_pred_proba = model.predict(X_test_processed).flatten()\n",
    "\n",
    "# Create calibration plot\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label='Calibration curve')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground Ball Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUND BALL MODEL\n",
    "gb_df = df[(df['is_gb'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Embedding, Flatten, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Simplified feature set with added 'hit_direction' and positional features\n",
    "features_simplified = [\n",
    "    'responsible_fielder', 'hit_direction',\n",
    "    'launch_speed', 'launch_angle',\n",
    "    'responsible_fielder_x', 'responsible_fielder_y', 'responsible_fielder_angle'\n",
    "]\n",
    "target = 'is_out'\n",
    "# Separate features and target\n",
    "X_simplified = gb_df[features_simplified].copy()\n",
    "y = gb_df[target]\n",
    "\n",
    "# Encode the categorical feature 'responsible_fielder'\n",
    "label_encoder = LabelEncoder()\n",
    "X_simplified['responsible_fielder'] = label_encoder.fit_transform(X_simplified['responsible_fielder'])\n",
    "\n",
    "# Define numeric features in the simplified set\n",
    "numeric_features_simplified = [\n",
    "    'hit_direction',\n",
    "    'launch_speed', 'launch_angle',\n",
    "    'responsible_fielder_x', 'responsible_fielder_y', 'responsible_fielder_angle'\n",
    "]\n",
    "\n",
    "# Preprocessing pipeline for the simplified numeric data\n",
    "numeric_transformer_simplified = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor_simplified = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_simplified, numeric_features_simplified)\n",
    "    ])\n",
    "\n",
    "# Split the data with the simplified features\n",
    "X_train_simplified, X_test_simplified, y_train, y_test = train_test_split(X_simplified, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to numeric features\n",
    "X_train_processed_simplified = preprocessor_simplified.fit_transform(X_train_simplified)\n",
    "X_test_processed_simplified = preprocessor_simplified.transform(X_test_simplified)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Embedding, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define input layers for numeric and categorical features\n",
    "numeric_input = Input(shape=(X_train_processed_simplified.shape[1],), name='numeric_input')\n",
    "embedded_fielder = Embedding(input_dim=len(label_encoder.classes_), output_dim=5, input_length=1)(X_train_simplified['responsible_fielder'])\n",
    "embedded_fielder = Flatten()(embedded_fielder)\n",
    "\n",
    "# Concatenate numeric and embedded features\n",
    "concatenated = concatenate([numeric_input, embedded_fielder])\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.0005)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.0005)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.0005)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_processed_simplified, y_train, epochs=110, batch_size=44, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_processed_simplified, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Plotting the training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
